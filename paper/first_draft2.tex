\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}




\title{}

\author{}


\begin{document}
	
	\maketitle
	
	
	\begin{abstract}
		
		

		
	\end{abstract}
	
	
	\section{Introduction}
	
	Many decisions that people are faced with require finding a balance between exploiting known information for short-term gain and exploring new sources of information that may lead to future reward. For example, a person might choose to go to a restaurant that they know well and have had multiple satisfying meals at, or choose to try a new restaurant that could result in a better or worse experience. This is known as the explore-exploit trade-off.
	
	Tasks requiring this kind of trade-off can be often be interpreted as multi-arm bandit (MAB) problems. A MAB problem consists of $A$ possible actions, where the $a$-th action yields the random reward $r_{t}$ drawn from an unknown distribution. Over $T$ trials, the agent must choose $a_{t:T}$ such that $\sum r_{t:T}$ is maximized. The policy $\tilde{\pi}$ maps the observed history of actions and rewards $h_{1:t-1} = [(a_{1}, r_{1}),...,(a_{t-1}, r_{t-1})]$ to the next action $a_{t}$ for all possible histories $h_{1:t-1} \in \mathcal{H}$. The goal in a MAB problem is to find the optimal policy such that the expected reward over $T$ trials, $\mathbf{E}[r_{1:T} | \tilde{\pi}( h_{1:t-1})]$, is maximized. This is the solution to the Bellman equation
	\begin{equation}
	\mathbf{E}[r_{1:T} | \tilde{\pi}( h_{1:t-1})] = \mathbf{E}[r_{t} | \tilde{\pi}(h_{1:t-1})] + \mathbf{E}[r_{t+1:T} | \tilde{\pi}( h_{1:t})]
	\end{equation}
	where the first term yields the expected reward on trial $t$ and the second term recursively defines the expected reward on all subsequent trials up to $T$. This second term can be evaluated for trial $t+1$ by weighing the expected reward given all possible histories $ \mathcal{ H}_{1:t} = [(a_{1}, r_{1}),...,(a_{t-1},r_{t-1}),(a_{t},r_{t})]$ for $r_{t} \in \mathcal{R}$ by the probability of that history occurring:
	\begin{equation}
	\mathbf{E}[r_{t+1:T} | \tilde{\pi}( h_{1:t})] = \sum_{r_{t} \in \mathcal{R}} p(r_{t} | a_{t}, h_{1:t-1}) \mathbf{E}[r_{1:T} | \tilde{\pi}( h_{1:t})]
	\end{equation}

	If the reward $r$ observed after choosing action $a$ is drawn from the distribution $H(\theta_{a})$, an agent must employ a strategy that balances choosing the action that maximizes the reward on trial $t$, $a_{t}^{*} = \text{argmax}_{a_{t} \in A} \mathbf{E}[H(\theta_{a_{t}})]$, and learning more about $\theta_{A}$ by choosing novel actions to maximize reward on future trials. As the problem of evaluating all possible histories is intractable, one of two general classes of approximations can be used. The first, simulation-based methods, involves using a subset of possible histories to evaluate the long-term reward of an action, and includes Thompson sampling \citep{Thompson} and Monte Carlo tree search \citep{Coulom2007}. Rather than relying on simulations, myopic strategies define a value function for approximating long-term reward. Common myopic strategies include upper confidence bound (UCB) \citep{Agrawal} and epsilon-greedy \citep{Sutton1998a} algorithms.
	
	Contextual multi-armed bandit (CMAB) problems introduce additional information into the standard MAB problem by way of a set of features associated with the set of possible examples \citep{Langford}. For example, a standard MAB formulation of the problem of choosing which restaurant to eat at assumes that the reward yielded by any two restaurants will be uncorrelated. However, it might be the case that these restaurants share a set of features (e.g. size, location, menu items) such that choosing similar restaurants can be assumed to yield similar rewards. Rather than having to execute an action to be able to evaluate its expected reward, considering shared features allows one to learn a function, $R: x_{t} \to r_{t}$ that maps features of the action $a_{t}$, $x_{a_{t}}$ to that action's expected reward, $r_{t}$.
	
	With the inclusion of context, CMAB problems require the additional step of learning the function $R$ between updating the history, $h_{1:t-1} = [(a_{1}, x_{1}, r_{1}),...,(a_{t-1}, x_{t-1}, r_{t-1})]$, and choosing $a_{t} = \tilde{\pi}(h_{t-1})$. Formally, function learning describes how people predict a continuous-valued output given an input, and can be thought of as a continuous extension of category learning. Theories of function learning typically follow either a rule-based or similarity-based approach. Rule-based approaches posit that people learn this mapping by assuming that the unknown function belongs to a particular parametric family, then inferring the most likely parameterization after observing input/output pairs. For example \cite{Carroll} considers polynomials up to degree 6, and \cite{Koh} consider power-law functions. While this approach attributes rich representations to learners, it not clear how these representations are acquired. Similarity-based theories suggest instead that learning is the result of forming associations between input/output pairs and generalizing these associations to similar inputs. \cite{Busemeyer2005LearningFR} implement a connectionist network where inputs activate a set of input nodes according to a Gaussian similarity function and each output node is activated according to learned weights between the input and output nodes. This approach does not require any assumptions about functional form and allows for flexible interpolation, but does not support generalization to inputs that are distant from past examples. While neither approach seems to fully capture human function learning, hybrid models have been introduced that take advantages from both rule and similarity-based theories. \cite{McDaniel2005TheCB} extend their connectionist model by including a layer of hidden nodes, each corresponding to a parameterization of a particular functional family. 
	
	More recently, \cite{Lucas} proposed Gaussian process regression (GRP) as a unified approach to function learning. GPR solves the problem of learning to map inputs to outputs by assuming that the outputs $\mathbf{y}_{N}$ are drawn from the $N$ dimensional distribution $\mathcal{N}(m(\mathbf{x}_{N}),k(\mathbf{x}_{N},\mathbf{x}_{N}))$, where $m$ defines a mean function that broadly captures the function in the absence of data, and $k$ defines how the inputs relate to each other. A common class of kernels, radial basis functions, e.g. $k(x_{i},x_{j}) = \sigma^{2}\exp(-\frac{(x_{i}-x_{j})^{2}}{2l^{2}})$, follows the assumption made by similarity-based models that similar inputs will map to similar outputs, with the parameter $l$ determining how quickly the correlation between two inputs decreases as the distance between them increases. Similarly, many rule-based models can be expressed in terms of the polynomial kernels $k(x_{i}, x_{j}) = \sigma^{2} (x_{i}x_{j} + c)^{d}$ of degree $d$. For the unobserved input $x^{*}$, the output $y^{*}$ is drawn from a normal distribution with the posterior mean and variance functions
	\begin{equation}
	\begin{split}
	& m(x^{*} | \mathbf{x}_{N}, \mathbf{y}_{N}) = m(x^{*}) + \mathbf{k}(x^{*})^{T}(\mathbf{K} + \sigma^{2} \mathbf{I}) (\mathbf{y}_{N}-m(\mathbf{x}_{N})) \\
	& v(x^{*} | \mathbf{x}_{N}, \mathbf{y}_{N}) = k(x^{*},x^{*}) - \mathbf{k}(x^{*})^{T}(\mathbf{K} + \sigma^{2} \mathbf{I})^{-1}\mathbf{k}(x^{*})
	\end{split}
	\end{equation}
	 where $\mathbf{k}(x^{*}) = [k(x_{1},x^{*}),...,k(x_{N},x^{*})]$ and $\mathbf{K} = k(\mathbf{x}_{N}, \mathbf{x}_{N})$.
	
	Bayesian optimization \citep{Snoek} provides a framework for solving CMAB problems when the reward function is assumed to be drawn from a Gaussian process. On each trial, the acquisition function $\text{acq}(a) = \text{acq}(m(a|h_{1:t-1}), v(a|h_{1:t-1}))$ is used to approximate the expected long-term reward of each action, and the next action is chosen according to the policy $\tilde{\pi}(h_{1:t-1}) = \text{argmax}_{a \in A}\text{acq}(a)$. Once the action/reward pair has been observed, the mean and variance functions at each action are updated. Of the commonly used acquisition functions, most follow the heuristic of favoring actions where both the mean and variance of the associated distribution over rewards are high; that is, where there is opportunity to either exploit a known reward or explore an unknown area of the reward function. The Gaussian process UCB acquisition function directly weighs exploration and exploitation by taking a weighted sum of the mean and variance of the reward function at $a$ 
	\begin{equation}
	\text{acq}_{UCB}(a) = m(a|h_{1:t-1}) + \beta \sqrt{v(a|h_{1:t-1})}
	\end{equation}
	where $\beta$ determines the preference for exploration. Another strategy is to maximize the probability of improving the current best reward:
	\begin{equation}
	\text{acq}_{PoI}(a) = \Phi \bigg( \frac{m(a|h_{1:t-1}) -  m(a*|h_{1:t-1})}{\sqrt{v(a|h_{1:t-1})}} \bigg)
	\end{equation}
	where $a^{*}$ is the action that is believed to maximize reward and $\Phi$ is the cumulative distribution function of the standard normal. A similar strategy is to maximize the expected improvement:
	\begin{equation}
	\text{acq}_{EI}(a) = (m(a|h_{1:t-1}) -  m(a^{*}|h_{1:t-1})) \Phi(z) + \sqrt{v(a|h_{1:t-1})}
	\end{equation}
	where $z = \frac{m(a|h_{1:t-1}) -  m(a^{*}|h_{1:t-1})}{\sqrt{v(a|h_{1:t-1})}}$.
	
	An alternative approach to maximizing this heuristic is to work directly with the distribution that describes the maximum of the function. Entropy search \citep{Hennig} achieves this by maximizing the expected change in entropy about $p(x^{*})$, where $x^{*} = \text{argmax}_{x \in X} R(x)$, yielded after choosing the action $a$:
	\begin{equation}
	\text{acq}_{ES}(a) = H(p(x^{*}|h_{1:t-1})) - \mathbf{E}[H(p(x^{*}|h_{1:t}))]
	\end{equation}
	and predictive entropy search \citep{Henrandez-Lobato} maximizes an equivalent value: 
	\begin{equation}
	\text{acq}_{PES}(a) = H(p(y|h_{1:t-1}, x)) - \mathbf{E}[H(p(y|h_{1:t-1}, x, x^{*}))]
	\end{equation}
	Max-value entropy search \cite{Wang} takes a slightly different approach, instead working to minimize entropy about $p(y^{*})$:
	\begin{equation}
	\text{acq}_{MES}(a) = H(p(y|h_{1:t-1}, x)) - \mathbf{E}[H(p(y|h_{1:t-1}, x, y^{*}))]
	\end{equation}
	While each of these strategies has the advantage of working with a global distribution over features and their associated rewards rather than a local heuristic, none of the required entropies can be computed analytically. However, a number of approximations have been proposed.
	
	Human learners' approach to the explore/exploit trade-off has typically been studied in tasks where the goal is to maximize cumulative reward \citep[e.g.][]{Bechara2005, SteyversLeeWagenmakers2009a, SchulzEmmanouilSpeekenbrink2017a}. However, the need for this trade-off can be observed in a number of other tasks. For instance, an agent might be instead interested in finding the maximum possible reward after $T$ trials. This problem is known as \textit{optimization} \citep[e.g.][]{}. In these cases, earning a high reward on any particular task $t$ is not important, as long as the actions lead to finding the global maximum. Another example is when an agent might instead be interested in learning the the reward function across all actions. These cases are described by \textit{active learning} (AL) \citep[e.g.][]{Bramley, BramleyGerstenbergTenenbaum2016a}. In typical learning paradigms learners are asked to make judgments based on evidence that has been preselected for them. In contrast, AL describes tasks where the learner plays a role in selecting the evidence to observe.
	
	While reinforcement learning, optimization, and active learning are often studied in isolation, there are cases where different tasks might rely on the same mapping between features and rewards. \textit{Multi-task learning} \citep[e.g.][]{} involves exploiting common structure among distinct tasks in order to learn each task more effectively.
	
	\section{Modeling the Explore/Exploit Trade-Off}
	
	We consider models of the three types of task requiring an explore/exploit trade-off that were described above. While the mapping from features to rewards, $R: x_{N} \to \mathcal{R}$, can be described in similar terms for all three tasks, their goal-specific reward, and thus the function used to approximate long-term reward, are distinct.
	
	Since the goal in RL tasks is to maximize cumulative reward over time, the goal-specific reward is the same as the reward, that is:
	\begin{equation}
	\mathbf{E}_{rl}[r'_{t}|a_{t}, h_{1:t-1}] = \mathbf{E}[r_{t}|a_{t}, h_{1:t-1}]
	\end{equation}
	In contrast, the goal of the optimization problem is simply to find the maximum possible reward withing $T$ steps. As such, the goal specific reward is defined as:
	\begin{equation}
	\mathbf{E}_{opt}[r'_{t}|a_{t}, h_{1:t-1}] = max(\mathbf{E}[r_{t}|a_{t}, h_{1:t-1}] - \sum_{i=1}^{t-1}r'_{i}, 0)
	\end{equation}
	That is, on trial if trial $t$ yields an increase in reward over the previous maximum reward, $r'_{t}$ is the difference. If not, $r'_{t}$ is 0. Since active learning is concerned with learning the reward function rather than the magnitude of the rewards themselves, its goal-specific reward can be described as the sum of the decrease in variance across all possible actions:
	\begin{equation}
	\mathbf{E}_{al}[r'_{t}|a_{t}, h_{1:t-1}] = \sum_{a \in A} \mathbf{Var}[r|a,h_{1:t-1}] - \mathbf{Var}[r|a,h_{1:t}]
	\end{equation}
	
	We hope to address three questions through the comparison of a number of models and there abilities to capture human performance on these tasks. First, we ask whether human learners act rationally with respect to information about the underlying reward function; that is, are people explicitly modeling the underlying reward function as an intermediate step in their decision making process, or relying on a less costly heuristic? To answer this question, we compare the class of Gaussian process-based strategies (e.g. GP UCB, probability of improvement, expected improvement, entropy search) with a model based on gradient descent. Second, we ask whether human learners act rationally with respect to their task-specific goal. In other words, do people use different strategies depending on their current goal, or rely on a more general measure of utility (e.g. choose actions with high expected reward and high uncertainty). We compare three task specific strategies with the general strategies of GP UCB, probability of improvement, expected improvement, and epsilon greedy. Third, we ask whether people adopt strategies that take into account higher-order information about the underlying reward function across multiple distinct tasks. We compare both strategies that include information about $R$ in their estimation of long-term reward across tasks and with those that consider only the long-term reward of the current task.
	
	\section{Experiments}
	
	In the following experiments, we consider sets of tasks where each relies on a similar mapping from features to rewards.
	
	\bibliographystyle{apacite}
	
	\setlength{\bibleftmargin}{.125in}
	\setlength{\bibindent}{-\bibleftmargin}
	
	\bibliography{first_draft}
		
	
\end{document}
