@article{SteyversLeeWagenmakers2009a,
	author =	 {M. Steyvers and M. D. Lee and E. -J. Wagenmakers},
	title =	 {A Bayesian analysis of human decision-making on bandit problems},
	journal =	 {Cognition and Brain Theory},
	year =	 2009,
	volume =	53,
	pages =	 {168--179}
}

@article {SchulzEmmanouilSpeekenbrink2017a,
	author = {Schulz, Eric and Konstantinidis, Emmanouil and Speekenbrink, Maarten},
	title = {Putting bandits into context: How function learning supports decision making},
	year = {2017},
	doi = {10.1101/081091},
	publisher = {Cold Spring Harbor Labs Journals},
	URL = {http://www.biorxiv.org/content/early/2017/06/15/081091},
	eprint = {http://www.biorxiv.org/content/early/2017/06/15/081091.full.pdf},
	journal = {bioRxiv}
}

@proceedings {BramleyGerstenbergTenenbaum2016a,
	title = {Natural science: Active learning in dynamic physical microworlds},
	journal = {38th Annual Meeting of the Cognitive Science Society},
	year = {2016},
	publisher = {38th Annual Meeting of the Cognitive Science Society},
	author = {Neil Bramley and Tobias Gerstenberg and Joshua B. Tenenbaum}
}

@article{Rachlin1981a,
	title={Maximization theory in behavioral psychology}, 
	volume={4}, 
	DOI={10.1017/S0140525X00009407}, 
	number={3}, 
	journal={Behavioral and Brain Sciences}, 
	publisher={Cambridge University Press}, 
	author={Rachlin, Howard and Battalio, Ray and Kagel, John and Green, Leonard}, 
	year={1981}, 
	pages={371â€“388}}

@inproceedings{Srinivas2010a,
	author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
	title = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	series = {ICML'10},
	year = {2010},
	isbn = {978-1-60558-907-7},
	location = {Haifa, Israel},
	pages = {1015--1022},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=3104322.3104451},
	acmid = {3104451},
	publisher = {Omnipress},
	address = {USA},
}

@book{Sutton1998a,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Introduction to Reinforcement Learning},
	year = {1998},
	isbn = {0262193981},
	edition = {1st},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@Inbook{Coulom2007,
	author="Coulom, R{\'e}mi",
	editor="van den Herik, H. Jaap
	and Ciancarini, Paolo
	and Donkers, H. H. L. M. (Jeroen)",
	title="Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search",
	bookTitle="Computers and Games: 5th International Conference, CG 2006, Turin, Italy, May 29-31, 2006. Revised Papers",
	year="2007",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="72--83",
	abstract="A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9{\texttimes}9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.",
	isbn="978-3-540-75538-8",
}


@INPROCEEDINGS{5035667, 
	author={B. E. Childs and J. H. Brodeur and L. Kocsis}, 
	booktitle={2008 IEEE Symposium On Computational Intelligence and Games}, 
	title={Transpositions and move groups in Monte Carlo tree search}, 
	year={2008}, 
	volume={}, 
	number={}, 
	pages={389-395}, 
	keywords={Monte Carlo methods;computer games;trees (mathematics);Monte Carlo tree search;artificial trees;effective branching factor;game programs;graph structure;upper confidence bounds;Algorithm design and analysis;Automation;Computer science;Electronic mail;History;Monte Carlo methods;Statistics;Tree data structures;Tree graphs}, 
	doi={10.1109/CIG.2008.5035667}, 
	ISSN={2325-4270}, 
	month={Dec},}

@inproceedings{Gelly,
	author = {Gelly, Sylvain and Silver, David},
	title = {Combining Online and Offline Knowledge in UCT},
	booktitle = {Proceedings of the 24th International Conference on Machine Learning},
	series = {ICML '07},
	year = {2007},
	isbn = {978-1-59593-793-3},
	location = {Corvalis, Oregon, USA},
	pages = {273--280},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/1273496.1273531},
	doi = {10.1145/1273496.1273531},
	acmid = {1273531},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@article{Agrawal,
	ISSN = {00018678},
	URL = {http://www.jstor.org/stable/1427934},
	abstract = {We consider a non-Bayesian infinite horizon version of the multi-armed bandit problem with the objective of designing simple policies whose regret increases slowly with time. In their seminal work on this problem, Lai and Robbins had obtained a O(log n) lower bound on the regret with a constant that depends on the Kullback-Leibler number. They also constructed policies for some specific families of probability distributions (including exponential families) that achieved the lower bound. In this paper we construct index policies that depend on the rewards from each arm only through their sample mean. These policies are computationally much simpler and are also applicable much more generally. They achieve a O(log n) regret with a constant that is also based on the Kullback-Leibler number. This constant turns out to be optimal for one-parameter exponential families; however, in general it is derived from the optimal one via a 'contraction' principle. Our results rely entirely on a few key lemmas from the theory of large deviations.},
	author = {Rajeev Agrawal},
	journal = {Advances in Applied Probability},
	number = {4},
	pages = {1054-1078},
	publisher = {Applied Probability Trust},
	title = {Sample Mean Based Index Policies with O(log n) Regret for the Multi-Armed Bandit Problem},
	volume = {27},
	year = {1995}
}


@article{Thompson,
	ISSN = {00063444},
	URL = {http://www.jstor.org/stable/2332286},
	author = {William R. Thompson},
	journal = {Biometrika},
	number = {3/4},
	pages = {285-294},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
	volume = {25},
	year = {1933}
}


@incollection{Langford,
	title = {The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information},
	author = {Langford, John and Zhang, Tong},
	booktitle = {Advances in Neural Information Processing Systems 20},
	editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
	pages = {817--824},
	year = {2008},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information.pdf}
}


@article {Carroll,
	author = {Carroll, J. Douglas},
	title = {FUNCTIONAL LEARNING: THE LEARNING OF CONTINUOUS FUNCTIONAL MAPPINGS RELATING STIMULUS AND RESPONSE CONTINUA},
	journal = {ETS Research Bulletin Series},
	volume = {1963},
	number = {2},
	issn = {2333-8504},
	url = {http://dx.doi.org/10.1002/j.2333-8504.1963.tb00958.x},
	doi = {10.1002/j.2333-8504.1963.tb00958.x},
	pages = {i--144},
	year = {1963},
}


@article{Koh,
	author = {Koh, Kyunghee and Meyer, David},
	year = {1991},
	month = {10},
	pages = {811-36},
	title = {Function Learning: Induction of Continuous Stimulus-Response Relations},
	volume = {17},
	booktitle = {Journal of experimental psychology. Learning, memory, and cognition}
}

@inproceedings{Busemeyer2005LearningFR,
	title={Learning Functional Relations Based on Experience With Input-Output Pairs by Humans and Artificial Neural Networks},
	author={Jerome R. Busemeyer and Eunhee Byun and Mark A. McDaniel},
	year={2005}
}


@article{McDaniel2005TheCB,
	title={The conceptual basis of function learning and extrapolation: comparison of rule-based and associative-based models.},
	author={Mark A. McDaniel and Jerome R. Busemeyer},
	journal={Psychonomic bulletin \& review},
	year={2005},
	volume={12},
	pages={24-42}
}


@incollection{Griffiths,
	title = {Modeling human function learning with Gaussian processes},
	author = {Thomas L. Griffiths and Lucas, Chris and Joseph Williams and Michael L. Kalish},
	booktitle = {Advances in Neural Information Processing Systems 21},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	pages = {553--560},
	year = {2009},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3529-modeling-human-function-learning-with-gaussian-processes.pdf}
}



@article{Lucas,
	author = {Lucas, Christopher and L Griffiths, Thomas and Williams, Joseph and Kalish, Michael},
	year = {2015},
	month = {03},
	title = {A rational model of function learning},
	volume = {22},
	booktitle = {Psychonomic bulletin \& review}
}


@inproceedings{Snoek,
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	title = {Practical Bayesian Optimization of Machine Learning Algorithms},
	booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
	series = {NIPS'12},
	year = {2012},
	location = {Lake Tahoe, Nevada},
	pages = {2951--2959},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2999325.2999464},
	acmid = {2999464},
	publisher = {Curran Associates Inc.},
	address = {USA},
} 


@article{Hennig,
	author = {Hennig, Philipp and Schuler, Christian J.},
	title = {Entropy Search for Information-efficient Global Optimization},
	journal = {J. Mach. Learn. Res.},
	issue_date = {January 2012},
	volume = {13},
	number = {1},
	month = jun,
	year = {2012},
	issn = {1532-4435},
	pages = {1809--1837},
	numpages = {29},
	url = {http://dl.acm.org/citation.cfm?id=2503308.2343701},
	acmid = {2343701},
	publisher = {JMLR.org},
	keywords = {Gaussian processes, expectation propagation, information, optimization, probability},
}


@inproceedings{Henrandez-Lobato,
	author = {Henr\'{a}ndez-Lobato, Jos{\'e} Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
	title = {Predictive Entropy Search for Efficient Global Optimization of Black-box Functions},
	booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
	series = {NIPS'14},
	year = {2014},
	location = {Montreal, Canada},
	pages = {918--926},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2968826.2968929},
	acmid = {2968929},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 


@InProceedings{Wang,
	title = 	 {Max-value Entropy Search for Efficient {B}ayesian Optimization},
	author = 	 {Zi Wang and Stefanie Jegelka},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {3627--3635},
	year = 	 {2017},
	editor = 	 {Doina Precup and Yee Whye Teh},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {International Convention Centre, Sydney, Australia},
	month = 	 {06--11 Aug},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/wang17e/wang17e.pdf},
	url = 	 {http://proceedings.mlr.press/v70/wang17e.html},
	abstract = 	 {Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the $\arg\max$ of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.}
}


@article{Bechara2005,
	title = {The Iowa Gambling Task and the Somatic Marker Hypothesis: Some Questions and Answers},
	author = {A. Bechara and H. Damasio and D. Tranel and A. R. Damasio},
	year = {2005},
	number = {4},
	journal = {Trends in Cognitive Sciences},
	pages = {159--162},
	volume = {9}
}


@article{Bramley,
	author = {Bramley, Lagnado and Speekenbrink},
	year = {2015},
	month = {5},
	pages = {708-31},
	title = {Conservative forgetful scholars: How people learn causal structure through sequences of interventions},
	volume = {41},
	booktitle = {Journal of experimental psychology. Learning, memory, and cognition}
}


@article{Tresp,
	title = {Mixtures of Gaussian Processes},
	author = {Volker Tresp},
	journal = {Advances in Neural Information Processing Systems 13},
	year = {2001},
}


@article {anderson,
	author = {Anderson, John},
	year = {1991},
	month = {07},
	pages = {409-429},
	title = {The Adaptive Nature Of Human Categorization},
	volume = {98},
	booktitle = {Psychological Review}
}

@article{Kushner1963,
	author = {Kushner, Harold},
	doi = {10.1109/JACC.1963.4168566},
	journal = {Joint Automatic Control Conference},
	language = {Undetermined},
	pages = {69 - 79},
	title = {A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise},
	volume = {1},
	year = {1963},
}


@incollection{krause,
	title = {Contextual Gaussian Process Bandit Optimization},
	author = {Krause, Andreas and Cheng S. Ong},
	booktitle = {Advances in Neural Information Processing Systems 24},
	editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
	pages = {2447--2455},
	year = {2011},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4487-contextual-gaussian-process-bandit-optimization.pdf}
}
