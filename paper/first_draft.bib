@article{SteyversLeeWagenmakers2009a,
  author =	 {M. Steyvers and M. D. Lee and E. -J. Wagenmakers},
  title =	 {A Bayesian analysis of human decision-making on bandit problems},
  journal =	 {Cognition and Brain Theory},
  year =	 2009,
  volume =	53,
  pages =	 {168--179}
}

@article {SchulzEmmanouilSpeekenbrink2017a,
	author = {Schulz, Eric and Konstantinidis, Emmanouil and Speekenbrink, Maarten},
	title = {Putting bandits into context: How function learning supports decision making},
	year = {2017},
	doi = {10.1101/081091},
	publisher = {Cold Spring Harbor Labs Journals},
	URL = {http://www.biorxiv.org/content/early/2017/06/15/081091},
	eprint = {http://www.biorxiv.org/content/early/2017/06/15/081091.full.pdf},
	journal = {bioRxiv}
}

@proceedings {BramleyGerstenbergTenenbaum2016a,
	title = {Natural science: Active learning in dynamic physical microworlds},
	journal = {38th Annual Meeting of the Cognitive Science Society},
	year = {2016},
	publisher = {38th Annual Meeting of the Cognitive Science Society},
	author = {Neil Bramley and Tobias Gerstenberg and Joshua B. Tenenbaum}
}

@article{Rachlin1981a,
	 title={Maximization theory in behavioral psychology}, 
	 volume={4}, 
	 DOI={10.1017/S0140525X00009407}, 
	 number={3}, 
	 journal={Behavioral and Brain Sciences}, 
	 publisher={Cambridge University Press}, 
	 author={Rachlin, Howard and Battalio, Ray and Kagel, John and Green, Leonard}, 
	 year={1981}, 
	 pages={371â€“388}}
 
@inproceedings{Srinivas2010a,
	author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
	title = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	series = {ICML'10},
	year = {2010},
	isbn = {978-1-60558-907-7},
	location = {Haifa, Israel},
	pages = {1015--1022},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=3104322.3104451},
	acmid = {3104451},
	publisher = {Omnipress},
	address = {USA},
}

@book{Sutton1998a,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	title = {Introduction to Reinforcement Learning},
	year = {1998},
	isbn = {0262193981},
	edition = {1st},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@Inbook{Coulom2007,
	author="Coulom, R{\'e}mi",
	editor="van den Herik, H. Jaap
	and Ciancarini, Paolo
	and Donkers, H. H. L. M. (Jeroen)",
	title="Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search",
	bookTitle="Computers and Games: 5th International Conference, CG 2006, Turin, Italy, May 29-31, 2006. Revised Papers",
	year="2007",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="72--83",
	abstract="A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to min-max as the number of simulations grows. This approach provides a fine-grained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9{\texttimes}9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.",
	isbn="978-3-540-75538-8",
}


@INPROCEEDINGS{5035667, 
	author={B. E. Childs and J. H. Brodeur and L. Kocsis}, 
	booktitle={2008 IEEE Symposium On Computational Intelligence and Games}, 
	title={Transpositions and move groups in Monte Carlo tree search}, 
	year={2008}, 
	volume={}, 
	number={}, 
	pages={389-395}, 
	keywords={Monte Carlo methods;computer games;trees (mathematics);Monte Carlo tree search;artificial trees;effective branching factor;game programs;graph structure;upper confidence bounds;Algorithm design and analysis;Automation;Computer science;Electronic mail;History;Monte Carlo methods;Statistics;Tree data structures;Tree graphs}, 
	doi={10.1109/CIG.2008.5035667}, 
	ISSN={2325-4270}, 
	month={Dec},}

@inproceedings{Gelly,
	author = {Gelly, Sylvain and Silver, David},
	title = {Combining Online and Offline Knowledge in UCT},
	booktitle = {Proceedings of the 24th International Conference on Machine Learning},
	series = {ICML '07},
	year = {2007},
	isbn = {978-1-59593-793-3},
	location = {Corvalis, Oregon, USA},
	pages = {273--280},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/1273496.1273531},
	doi = {10.1145/1273496.1273531},
	acmid = {1273531},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@article{Agrawal,
	ISSN = {00018678},
	URL = {http://www.jstor.org/stable/1427934},
	abstract = {We consider a non-Bayesian infinite horizon version of the multi-armed bandit problem with the objective of designing simple policies whose regret increases slowly with time. In their seminal work on this problem, Lai and Robbins had obtained a O(log n) lower bound on the regret with a constant that depends on the Kullback-Leibler number. They also constructed policies for some specific families of probability distributions (including exponential families) that achieved the lower bound. In this paper we construct index policies that depend on the rewards from each arm only through their sample mean. These policies are computationally much simpler and are also applicable much more generally. They achieve a O(log n) regret with a constant that is also based on the Kullback-Leibler number. This constant turns out to be optimal for one-parameter exponential families; however, in general it is derived from the optimal one via a 'contraction' principle. Our results rely entirely on a few key lemmas from the theory of large deviations.},
	author = {Rajeev Agrawal},
	journal = {Advances in Applied Probability},
	number = {4},
	pages = {1054-1078},
	publisher = {Applied Probability Trust},
	title = {Sample Mean Based Index Policies with O(log n) Regret for the Multi-Armed Bandit Problem},
	volume = {27},
	year = {1995}
}


@article{Thompson,
	ISSN = {00063444},
	URL = {http://www.jstor.org/stable/2332286},
	author = {William R. Thompson},
	journal = {Biometrika},
	number = {3/4},
	pages = {285-294},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
	volume = {25},
	year = {1933}
}


@incollection{Langford,
	title = {The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information},
	author = {Langford, John and Zhang, Tong},
	booktitle = {Advances in Neural Information Processing Systems 20},
	editor = {J. C. Platt and D. Koller and Y. Singer and S. T. Roweis},
	pages = {817--824},
	year = {2008},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information.pdf}
}


@article {Carroll,
	author = {Carroll, J. Douglas},
	title = {FUNCTIONAL LEARNING: THE LEARNING OF CONTINUOUS FUNCTIONAL MAPPINGS RELATING STIMULUS AND RESPONSE CONTINUA},
	journal = {ETS Research Bulletin Series},
	volume = {1963},
	number = {2},
	issn = {2333-8504},
	url = {http://dx.doi.org/10.1002/j.2333-8504.1963.tb00958.x},
	doi = {10.1002/j.2333-8504.1963.tb00958.x},
	pages = {i--144},
	year = {1963},
}


@article{Koh,
	author = {Koh, Kyunghee and Meyer, David},
	year = {1991},
	month = {10},
	pages = {811-36},
	title = {Function Learning: Induction of Continuous Stimulus-Response Relations},
	volume = {17},
	booktitle = {Journal of experimental psychology. Learning, memory, and cognition}
}

@inproceedings{Busemeyer2005LearningFR,
	title={Learning Functional Relations Based on Experience With Input-Output Pairs by Humans and Artificial Neural Networks},
	author={Jerome R. Busemeyer and Eunhee Byun and Mark A. McDaniel},
	year={2005}
}